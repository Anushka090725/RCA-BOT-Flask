# print("prompt_builder.py")
def build_prompt(metrics, logs, kb_docs, app_metrics,grafana_alerts):
    """
    Build a detailed prompt for the Gemini model using input metrics, logs, and retrieved KB documents.
    """
    return f"""
You are an expert DevOps troubleshooting assistant.


Here are the alerts of Grafana:
{grafana_alerts}

Here are the system metrics:
{metrics}

Here are the related logs:
{logs}
Here are the metrics of the application:
{app_metrics}
Here are some relevant documents from the Knowledge Base:
{kb_docs}


Using the above data, identify:
1. The most probable root cause of the issue.
2. Suggested remediation steps to resolve it. Complete them in 50 words.
3. Any preventive actions to avoid similar incidents in future. Complete them in 50 words.
4. Make it intresting to read with adding the relevant emojis.
5. Keep it crisp and clear in the format.

Sending you the example of the mail format: 
ğŸ“© Subject: RCA Report ğŸ§  | Root Cause Analysis Summary for [Alert Name or Instance]
ğŸš¨ Alert Triggered:
ğŸ”¹ Source: Grafana
ğŸ”¹ Instance: flask-app:8000
ğŸ”¹ Timestamp: 2025-07-27 14:32 IST
ğŸ”¹ Severity: âš ï¸ High

ğŸ§  1. Probable Root Cause
ğŸ” Based on the observed metrics and logs, the issue is likely due to [insert root cause].
Metrics such as CPU usage spike (92%) and response time delay indicate [explanation].

ğŸ› ï¸ 2. Suggested Remediation (within 50 words)
â¡ï¸ Restart the affected pod/service and scale the replica count.
â¡ï¸ Optimize the application route handler to avoid blocking operations.
â¡ï¸ Use resource limits in deployment YAML.

ğŸ›¡ï¸ 3. Preventive Actions (within 50 words)
âœ… Add auto-scaling policies in Kubernetes.
âœ… Configure alert thresholds with better granularity.
âœ… Regularly monitor health endpoints using blackbox exporters.
âœ… Set up canary deployments for safer updates.

ğŸ“Š Insightful Metrics at the Time of Alert
cpu_usage: 92% ğŸš¨

response_time: 5s ğŸ¢

memory_usage: 81%

Grafana Dashboard :http://65.1.84.70:3000/d/pS6ZuGV7z/prometheus-blackbox-exporter?orgId=1 

status: down âŒ

ğŸ“š Relevant Knowledge Base Reference
ğŸ“ Pulled insights from:
/kb/scaling_pods.md, /kb/http_latency_issues.txt
"""
# Here are some relevant documents from the Knowledge Base:
# {kb_docs}
